

üß† Prompt: Build a Cloudflare AI Proxy Worker (OpenAI-Compatible)

You are building a TypeScript-based Cloudflare Worker that serves as an OpenAI-compatible AI proxy with the following specifications:

‚∏ª

‚úÖ Purpose
	‚Ä¢	Serve OpenAI-compatible endpoints like /v1/chat/completions, /v1/completions, /v1/tokenize
	‚Ä¢	Default to Cloudflare Workers AI, using Meta LLaMA 4 Scout (@cf/meta/llama-4-scout-17b-16e-instruct)
	‚Ä¢	Optionally offload to OpenAI or Gemini using an AI Gateway pattern
	‚Ä¢	Allow specifying the provider and model via JSON payload (e.g., provider, model)
	‚Ä¢	Hide differences from the client ‚Äî return OpenAI-compatible responses regardless of backend

‚∏ª

‚öôÔ∏è Core Features
	‚Ä¢	Provider Registry: Central config for available models per provider
	‚Ä¢	Default behavior: Cloudflare AI + LLaMA 4
	‚Ä¢	AI Gateway: Logic to route to OpenAI or Gemini if requested
	‚Ä¢	No context chunking required for Workers AI (LLaMA 4 has 131k token window)
	‚Ä¢	Token estimation endpoint (/v1/tokenize) using tiktoken or fast fallback
	‚Ä¢	Logging layer (optional): D1-based audit trail of requests
	‚Ä¢	Session support: Accept session ID via cookies or headers for request tracking
	‚Ä¢	Secure by default: Requires X-AUTH-TOKEN or uses CORS/key-based policy

‚∏ª

üîå Input Schema (for /v1/chat/completions)

Accepts:

{
  "model": "@cf/meta/llama-4-scout-17b-16e-instruct",
  "provider": "cloudflare", // or "openai", "gemini"
  "messages": [
    { "role": "system", "content": "You are a helpful assistant." },
    { "role": "user", "content": "Summarize this page." }
  ],
  "temperature": 0.7,
  "stream": false
}


‚∏ª

üß© Model Registry

const MODEL_REGISTRY = {
  cloudflare: {
    default: "@cf/meta/llama-4-scout-17b-16e-instruct",
    llama4: "@cf/meta/llama-4-scout-17b-16e-instruct",
  },
  openai: {
    default: "gpt-4",
    gpt4: "gpt-4",
    gpt3: "gpt-3.5-turbo",
  },
  gemini: {
    default: "gemini-pro",
  }
};


‚∏ª

üì¶ Endpoints

Endpoint	Description
/v1/chat/completions	OpenAI-style chat interface (all providers)
/v1/completions	Optional legacy support (Codex-style)
/v1/tokenize	Estimates tokens (via tiktoken or fallback)
/v1/model-options	Returns supported models & metadata
/v1/route-check	Returns active route + model (debug endpoint)


‚∏ª

üõ°Ô∏è Notes
	‚Ä¢	This Worker will not scrape or violate ToS ‚Äî it‚Äôs used for AI routing only.
	‚Ä¢	Designed to be called by other Workers or services (e.g., your price tracker VM)
	‚Ä¢	AI gateway should cleanly handle rate limits, retries, and provider fallbacks

‚∏ª

Would you like me to:
	‚Ä¢	Generate a full wrangler.toml
	‚Ä¢	Scaffold the src/index.ts
	‚Ä¢	Add d1.ts and ai-router.ts
	‚Ä¢	Include a CLI test harness (curl + run.sh)
	‚Ä¢	Auto-deploy preview URL for testing?

Let me know and I‚Äôll push the complete project structure for you.